<!doctype html>
<html lang="en" color-scheme="light dark">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="./assets/style.css">

    <title>mon's blog ðŸ‘¾</title>
</head>

<body>
    <div id="content">
        <ul class="menu-bar">
            <li class="menu-item" id="posts">
                <a class="dropdown-btn" href="#" title="Posts">
                    Posts
                </a>
                <div class="dropdown-content">
                    <a class="dropdown-item" href="./posts/2026.html">2026</a>
                    <a class="dropdown-item" href="./posts/2025.html">2025</a>
                    <a class="dropdown-item" href="./posts/2024.html">2024</a>
                </div>
            </li>
        </ul>
        <br />
    </div>
    <h3>Local cluster access (08/25)</h3>
    <p>I wanted to add a post with a lot of details and little rant for some time about how complicated is to do
        something that should just work, but well, this is our reality, regardless how much I like it or not. So if you
        ever wonder how to access something like <em>argo</em> in a <em>kubernetes</em> cluster managed by
        <em>podman</em> without any <code>port-forward</code> voodoo and you do this for experimentation purposes, well,
        I think I found a formula.</p>
    <p>I&rsquo;m not gonna lie, I spent countless hours trying to find the right approach. I even asked <em>AIs</em> for
        help and, of course, still failed. In the end, after tinkering, the solution is simpler than what you might
        think.</p>
    <p>Adding a <em>kubernetes</em> cluster in your local machine, to me had 2 common flavors: <em>minikube</em> and
        <em>k3d</em> (this is <em>k3s</em> optimized for <em>docker</em>). I knew nothing about <em>kind</em> when I
        started this, I know <em>minikube</em> is a pain in the ass (too raw), and I read <em>k3d&rsquo;s</em> warning:
        <em>podman</em> is not supported. So I had to opt for an alternative and well, <em>podman</em> suggests using
        <em>kind</em>.</p>
    <p>The first challenge was easy to overcome, when you try to start a cluster (either using <em>k3d</em> or
        <em>kind</em>), <em>podman</em> will croak because it wants to use a rootless access, well, <em>podman</em>
        replaces <em>docker</em> and if you were smart enough to install <code>podman-docker</code> extension (mimicking
        <em>docker</em> commands), you just need to give it access to the <code>podman.sock</code> socket. To do this,
        just mind adjusting permissions or adding whatever runs <em>podman</em> to the same group that owns
        <code>/run/podman/podman.sock</code>. First I tried adjusting this using env variables, but the results was
        poor, the easiest way to get <em>kind</em> to start a cluster is doing this, just be wary on the permission
        settings.</p>
    <p>As I moved on, I found pretty simple to start and add services to my cluster by using <em>helm</em>, however,
        exposing ports is <em>nightmarish</em>, the reason for that is that <em>kind</em> runs its
        <em>control-plane</em> in a container. You cannot just change a container without recreating it, so you need to
        give some thought on what you will expose, that&rsquo;s not something coming as default and will feel
        uncomfortable from the <em>GUI</em>. This however, is key to forwarding traffic, so if you mean to let your
        services be accessed without using port-forwarding, you need to at least expose a port through your config
        (<code>config.yaml</code>):</p>
    <pre>
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 80
  ...
  - containerPort: 30000
    hostPort: 8088
    listenAddress: "0.0.0.0"
</pre>

    <p>Adding this to the list of your port-mapping will spare you a tons of headaches. Adding it from the <em>GUI</em>
        was not possible for me though (I tried using the config field and it did nothing!), so you will need to
        delegate this to <code>systemd</code>. Why <code>systemd</code>, what did I miss? Well, rootless is a premise of
        kubernetes in <em>podman</em>, something that I think, they strived for when <em>podman</em> was offered as the
        alternative to <em>docker</em> (<em>docker</em> has evolved since, but well&hellip;) so in order to start a
        service, you will need to do it at user level scope:</p>
    <p><code>/usr/bin/systemd-run --scope --user -p "Delegate=yes" kind create cluster --name my-kindiest-cluster --config config.yaml</code>
    </p>
    <p>Once you have this mapping, basically you defined that whatever that is bound to port <code>30000</code> will be
        accessed through <code>8088</code> in the host machine. This is necessary for your service which you will later
        patch.</p>
    <p>From here on, installing a service, is just a matter of running <code>helm</code> and installing things:</p>
    <pre>
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update
helm install argocd argo/argo-cd
</pre>

    <p>In a perfect world you would be done here. Maybe, you would want to add an ingress to forward traffic to a path
        so your node can serve more services (this <strong>never</strong> worked for me), but one thing you will learn,
        is that there are no happy paths in <em>kubernetes</em>. So having no ingress and being unable to access your
        service, what then?</p>
    <p>If you ask <em>Gemini</em> or <em>ChatGPT</em> the solution is using the <code>port-forward</code> subcommand,
        but do you really want to hijack a terminal with a command output? Well, forwarding should be permanent, but it
        isn&rsquo;t, however, you can patch the mapping, in this case, <code>8088</code> is already set to get traffic
        from port <code>30000</code>; this port is in the range of <code>NodePort</code> and we want to map our service
        to it, this is needed, since we will direct our application to what is accessible according to the
        <em>kubernetes API</em>:</p>
    <p><code>kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "NodePort", "ports": [{"port": 443, "targetPort": 8080, "protocol": "TCP", "name": "https", "nodePort": 30000}]}}'</code>
    </p>
    <p>I gotta be honest here, I despise that services and orchestrators alike think that ports <code>80</code>,
        <code>443</code> and <code>8080</code> are just there available. This is the part of the work where development
        seems just idiotic, but well, in this case, patching will tell my service that whatever it&rsquo;s configured to
        use port <code>8080</code> (which I don&rsquo;t want it to use, but it is its default) will get traffic from
        port <code>30000</code>, this mapping will allow me to use the other mapping I defined (container level
        mapping), thus, accessing my browser at <code>https://127.0.0.1:8088</code> will get me into my service landing
        page.</p>
    <p>There&rsquo;s a lot of juggling in this and well, it could be worse, we could add more services and then find
        that we need a longer list of port mappings in the control-plane. We could add proxying and an ingress to give
        it some sense of order, but that&rsquo;s just naive, because, as I said, there is no happy path.</p>
    <p>Asking this to <em>LLMs</em> feels like asking this to reddit&rsquo;s <em>kubernetes</em> evangelists:
        <em>&ldquo;this is better because excuse, excuse, excuse&hellip;&rdquo;</em>. I did some further reading, and I
        learned that in the &ldquo;real&rdquo; world, where this is run in multiple nodes, my setup would have less
        friction, or at least less mapping here and there. Would it? I stated it at the beginning: I don&rsquo;t mean to
        rant, <strong>my service is accessible!</strong> However, I think all these stunts could just be replaced by
        running a bunch of services and an <em>nginx</em> instance without the <em>yaml</em> and layers non-sense, or
        even better, by letting what has been proved to work for many decades take over (yes, I mean <em>DHCP</em>,
        <em>DNS</em>, <em>NAT</em>, which, by the way, have had their working equivalents in the public cloud for close
        to 2 decades already).</p>
    <p>However, these systems will come with their own inconveniences, right? Yes! We could add proxy clusters and
        health-checks, define a load balancing strategy and autoscaling mechanisms and well, end up with the same kind
        of problem; I think <em>kubernetes</em> is fixing an inconvenience and creating another one, but I understand
        its purpose, a noble one, that could however, be fixed in a well controlled manner, e.g. from the application
        layer itself. I admit, I would be blindsided by evaluating <em>kubernetes</em> just by a single experiment on a
        single host for a single instance of a single application, so just be mindful on your needs before you embark
        into this journey. Call it and place it wherever you want, layers and complexity are a trade-off that, at least
        in my opinion, should not be adopted before the need arises. </p>
    <p><strong><em>tl;dr</em> - Don&rsquo;t mind having a container orchestrator infrastructure when you don&rsquo;t
            need more than one instance, focus instead on configuring things properly.</strong></p>
    <p>I don&rsquo;t see the value in convoluting systems with the lame excuse of scale; most of our foundation services
        can handle a fair volume of traffic just fine and doing things for a scale that never comes is just foolish, but
        I think that&rsquo;s a conversation for another day.</p>
</body>

</html>