<!doctype html>
<html lang="en" color-scheme="light dark">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="./assets/style.css">

    <title>mon's blog ğŸ‘¾</title>
</head>

<body>
    <div id="content">
        <ul class="menu-bar">
            <li class="menu-item" id="posts">
                <a class="dropdown-btn" href="#" title="Posts">
                    Posts
                </a>
                <div class="dropdown-content">
                    <a class="dropdown-item" href="./posts/2026.html">2026</a>
                    <a class="dropdown-item" href="./posts/2025.html">2025</a>
                    <a class="dropdown-item" href="./posts/2024.html">2024</a>
                </div>
            </li>
        </ul>
        <br />
    </div>
    <h1>Building a Production-Ready Flask Observability Stack with OpenTelemetry</h1>
    <p>Before we get into the weeds; this one was written using Claude (Sonnet 4.5). I think it does a decent job in
        many ways; I asked it for a tutorial on how to setup this stack (I have done it before, so I can verify against
        a live version of it), and while maybe not all the code and configurations will work (it took the model 13
        revisions to come up with this markdown), I think it gets pretty close to the results I wanted to see.</p>
    <h2>Problem Statement</h2>
    <p>Modern web applications require comprehensive observability to maintain reliability at scale. However,
        implementing metrics, traces, and logs creates several challenges:</p>
    <ol>
        <li><strong>Performance overhead</strong>: Telemetry collection can become a bottleneck, especially synchronous
            database logging</li>
        <li><strong>Integration complexity</strong>: Different formats for metrics (Prometheus), traces (Jaeger), and
            logs (Elasticsearch)</li>
        <li><strong>Scalability limits</strong>: Traditional architectures struggle beyond a few hundred requests per
            second</li>
        <li><strong>Load testing constraints</strong>: Python-based tools hit protocol limitations around 500-600 QPS
            per machine</li>
        <li><strong>Database bottlenecks</strong>: Write-heavy logging workloads can saturate database connections</li>
    </ol>
    <h2>Solution Architecture</h2>
    <p>We&rsquo;ll build a decoupled, horizontally scalable architecture that separates concerns and eliminates
        bottlenecks:</p>
    <pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Load Testing Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Locust  â”‚  â”‚  vegeta  â”‚  â”‚  wrk2    â”‚  â”‚  Gatling â”‚       â”‚
â”‚  â”‚  Worker  â”‚  â”‚   (Go)   â”‚  â”‚   (C)    â”‚  â”‚  (JVM)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       HAProxy         â”‚
              â”‚   Load Balancer       â”‚
              â”‚   (Round Robin)       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬
        â”‚             â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  App   â”‚   â”‚  App   â”‚   â”‚  App   â”‚   â”‚  App   â”‚
   â”‚Instanceâ”‚   â”‚Instanceâ”‚   â”‚Instanceâ”‚   â”‚Instanceâ”‚
   â”‚   1    â”‚   â”‚   2    â”‚   â”‚   3    â”‚   â”‚   4    â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚            â”‚            â”‚            â”‚
       â”‚ Emit Telemetry          â”‚            â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
       â”‚                                      â”‚
       â”‚         Queue Events                 â”‚
       â–¼                                      â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Redis  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  OTEL        â”‚
   â”‚ Queue  â”‚                          â”‚  Collector   â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                      â”‚
       â”‚                           â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬
       â”‚ Consume                   â”‚          â”‚          â”‚
       â–¼                           â–¼          â–¼          â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Worker â”‚                â”‚Jaeger   â”‚ â”‚Prom.   â”‚ â”‚Elastic-  â”‚
   â”‚   1    â”‚                â”‚(Traces) â”‚ â”‚(Metricsâ”‚ â”‚search    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚(Logs)    â”‚
   â”‚ Worker â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚   2    â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Worker â”‚
   â”‚   3    â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Batch Write
       â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  PostgreSQL    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Replica 1   â”‚
   â”‚    Primary     â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  (Writes Only) â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Replica 2   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â”‚  Replica 3   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â”‚ Read Queries
                                     â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  App         â”‚
                              â”‚  Instances   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>

    <p><strong>Architecture Principles:</strong></p>
    <ol>
        <li><strong>Async writes</strong>: Redis queue decouples request handling from database I/O</li>
        <li><strong>Batch processing</strong>: Workers bulk-insert 100+ events per transaction</li>
        <li><strong>Read/Write separation</strong>: Primary for writes only, replicas for all reads</li>
        <li><strong>Distributed telemetry</strong>: OTEL collector routes to specialized backends</li>
        <li><strong>Multi-language load testing</strong>: Overcome Python&rsquo;s ~600 QPS/machine limit</li>
    </ol>
    <h2>Project Structure</h2>
    <pre>
visitor-tracker/
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ models.py                       # SQLAlchemy models with ULID
â”œâ”€â”€ database.py                     # Database connection management (R/W split)
â”œâ”€â”€ telemetry.py                    # OpenTelemetry setup
â”œâ”€â”€ worker.py                       # Async database writer
â”œâ”€â”€ pyproject.toml                  # UV project configuration
â”œâ”€â”€ uv.lock                         # Locked dependencies
â”œâ”€â”€ hypercorn.toml                  # ASGI server configuration
â”œâ”€â”€ Dockerfile                      # Container image
â”œâ”€â”€ requirements.txt                # Generated from uv
â”œâ”€â”€ alembic/                        # Database migrations
â”‚   â”œâ”€â”€ env.py
â”‚   â”œâ”€â”€ versions/
â”‚   â””â”€â”€ alembic.ini
â”œâ”€â”€ config/                         # Service configurations
â”‚   â”œâ”€â”€ haproxy.cfg                 # Load balancer config
â”‚   â”œâ”€â”€ otel-collector-config.yaml # OTEL collector config
â”‚   â”œâ”€â”€ prometheus.yml              # Prometheus config
â”‚   â””â”€â”€ postgresql.conf             # PostgreSQL tuning
â”œâ”€â”€ load-testing/                   # Load testing suite
â”‚   â”œâ”€â”€ locustfile.py              # Python-based tests (baseline)
â”‚   â”œâ”€â”€ vegeta-targets.txt         # Go-based load tests
â”‚   â”œâ”€â”€ wrk2-script.lua            # C-based constant-rate tests
â”‚   â””â”€â”€ gatling/                   # JVM-based scenario tests
â”‚       â””â”€â”€ VisitorTrackerSimulation.scala
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ scaling-guide.md
    â””â”€â”€ runbook.md
</pre>

    <h2>Problem Statement</h2>
    <h2>Project Setup with UV</h2>
    <p>We&rsquo;ll use UV, the fast Python package manager, to set up our project with Python 3.14.</p>
    <pre><code>
# Install UV if you haven't already
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create project
uv init visitor-tracker
cd visitor-tracker

# Set Python version
uv python pin 3.14

# Add dependencies
uv add flask sqlalchemy alembic psycopg2-binary python-ulid hypercorn \
    opentelemetry-api opentelemetry-sdk \
    opentelemetry-instrumentation-flask \
    opentelemetry-instrumentation-sqlalchemy \
    opentelemetry-exporter-otlp-proto-grpc \
    opentelemetry-exporter-otlp-proto-http

# Add development dependencies
uv add --dev locust pytest

# Add Redis for async queue
uv add redis
</code></pre>

    <h2>Database Model with ULID</h2>
    <p>Create the database model using SQLAlchemy with ULID as the primary key for better distributed system
        compatibility.</p>
    <p><strong><code>models.py</code>:</strong></p>
    <pre><code>
from datetime import datetime, timezone
from sqlalchemy import Column, String, Integer, DateTime, create_engine
from sqlalchemy.orm import declarative_base
from ulid import ULID

Base = declarative_base()


class VisitorEvent(Base):
    __tablename__ = "visitor_events"

    id = Column(String(26), primary_key=True, default=lambda: str(ULID()))
    timestamp = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc))
    source_ip = Column(String(45), nullable=False)  # IPv6 support
    endpoint = Column(String(255), nullable=False)
    method = Column(String(10), nullable=False)
    response_code = Column(Integer, nullable=False)
    user_agent = Column(String(512), nullable=True)

    def __repr__(self):
        return f"<VisitorEvent(id={self.id}, endpoint={self.endpoint}, method={self.method})>"
</code></pre>

    <p><strong>Initialize Alembic:</strong></p>
    <pre><code>
uv run alembic init alembic

# Edit alembic.ini to set your database URL
# sqlalchemy.url = postgresql://user:password@postgres:5432/visitor_tracker
</code></pre>

    <p><strong><code>alembic/env.py</code></strong> (update the target_metadata):</p>
    <pre><code>
from models import Base
target_metadata = Base.metadata
</code></pre>

    <p><strong>Create initial migration:</strong></p>
    <pre><code>
uv run alembic revision --autogenerate -m "Create visitor_events table"
uv run alembic upgrade head
</code></pre>

    <h2>OpenTelemetry Instrumentation</h2>
    <p>Set up comprehensive OpenTelemetry instrumentation for metrics, traces, and logs.</p>
    <p><strong><code>telemetry.py</code>:</strong></p>
    <pre><code>
import logging
from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.sdk.resources import Resource, SERVICE_NAME
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
from opentelemetry.exporter.otlp.proto.grpc._log_exporter import OTLPLogExporter
from opentelemetry.sdk._logs import LoggerProvider, LoggingHandler
from opentelemetry.sdk._logs.export import BatchLogRecordProcessor
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor


def setup_telemetry(app, service_name="visitor-tracker", otlp_endpoint="http://otel-collector:4317"):
    """Initialize OpenTelemetry instrumentation for Flask application"""

    resource = Resource(attributes={SERVICE_NAME: service_name})

    # Setup Tracing
    trace_provider = TracerProvider(resource=resource)
    trace_provider.add_span_processor(
        BatchSpanProcessor(OTLPSpanExporter(endpoint=otlp_endpoint, insecure=True))
    )
    trace.set_tracer_provider(trace_provider)

    # Setup Metrics
    metric_reader = PeriodicExportingMetricReader(
        OTLPMetricExporter(endpoint=otlp_endpoint, insecure=True),
        export_interval_millis=30000  # Export every 30 seconds
    )
    meter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])
    metrics.set_meter_provider(meter_provider)

    # Setup Logging
    logger_provider = LoggerProvider(resource=resource)
    logger_provider.add_log_record_processor(
        BatchLogRecordProcessor(OTLPLogExporter(endpoint=otlp_endpoint, insecure=True))
    )
    handler = LoggingHandler(level=logging.INFO, logger_provider=logger_provider)
    logging.getLogger().addHandler(handler)
    logging.getLogger().setLevel(logging.INFO)

    # Auto-instrument Flask
    FlaskInstrumentor().instrument_app(app)

    # Get meter for custom metrics
    meter = metrics.get_meter(__name__)

    return {
        "tracer": trace.get_tracer(__name__),
        "meter": meter,
        "request_counter": meter.create_counter(
            "http_requests_total",
            description="Total HTTP requests",
            unit="1"
        ),
        "request_duration": meter.create_histogram(
            "http_request_duration_seconds",
            description="HTTP request duration",
            unit="s"
        ),
        "db_operations": meter.create_counter(
            "db_operations_total",
            description="Total database operations",
            unit="1"
        )
    }
</code></pre>

    <h2>Flask Application</h2>
    <p>Create the main Flask application with comprehensive telemetry.</p>
    <p><strong><code>app.py</code>:</strong></p>
    <pre><code>
import logging
import time
from datetime import datetime, timezone
from flask import Flask, request, jsonify
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, scoped_session
from models import Base, VisitorEvent
from telemetry import setup_telemetry
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor
import os

# Configuration
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://tracker:tracker@postgres:5432/visitor_tracker")
OTLP_ENDPOINT = os.getenv("OTLP_ENDPOINT", "http://otel-collector:4317")

# Initialize Flask
app = Flask(__name__)
app.config["DATABASE_URL"] = DATABASE_URL

# Setup database
engine = create_engine(DATABASE_URL, pool_size=20, max_overflow=40, pool_pre_ping=True)
Base.metadata.create_all(engine)
session_factory = sessionmaker(bind=engine)
Session = scoped_session(session_factory)

# Instrument SQLAlchemy
SQLAlchemyInstrumentor().instrument(
    engine=engine,
    service="visitor-tracker-db"
)

# Setup OpenTelemetry
telemetry = setup_telemetry(app, service_name="visitor-tracker", otlp_endpoint=OTLP_ENDPOINT)
log = logging.getLogger(__name__)


def get_client_ip():
    """Extract client IP from request, handling proxies"""
    if request.headers.get('X-Forwarded-For'):
        return request.headers.get('X-Forwarded-For').split(',')[0].strip()
    elif request.headers.get('X-Real-IP'):
        return request.headers.get('X-Real-IP')
    return request.remote_addr


@app.before_request
def before_request():
    """Track request start time"""
    request.start_time = time.time()


@app.after_request
def after_request(response):
    """Log request details and emit metrics"""
    duration = time.time() - request.start_time

    # Emit metrics
    attributes = {
        "endpoint": request.endpoint or "unknown",
        "method": request.method,
        "status_code": response.status_code
    }
    telemetry["request_counter"].add(1, attributes)
    telemetry["request_duration"].record(duration, attributes)

    # Queue event for async processing instead of synchronous DB write
    # This prevents the database from becoming a bottleneck
    event_data = {
        "timestamp": datetime.now(timezone.utc),
        "source_ip": get_client_ip(),
        "endpoint": request.endpoint or request.path,
        "method": request.method,
        "response_code": response.status_code,
        "user_agent": request.headers.get('User-Agent', '')
    }

    try:
        # Push to Redis queue for async processing
        import redis
        r = redis.Redis(host='redis', port=6379, decode_responses=False)
        import json
        r.lpush('visitor_events', json.dumps(event_data, default=str))
        log.info(f"Queued visit: {request.method} {request.path} from {get_client_ip()}")
    except Exception as e:
        log.error(f"Failed to queue visit: {e}", exc_info=True)

    return response


@app.route("/")
def index():
    """Home endpoint"""
    return jsonify({
        "message": "Visitor Tracker API",
        "version": "1.0.0",
        "endpoints": ["/", "/health", "/stats", "/visits"]
    })


@app.route("/health")
def health():
    """Health check endpoint"""
    try:
        session = Session()
        session.execute("SELECT 1")
        Session.remove()
        return jsonify({"status": "healthy", "database": "connected"}), 200
    except Exception as e:
        log.error(f"Health check failed: {e}")
        return jsonify({"status": "unhealthy", "error": str(e)}), 503


@app.route("/stats")
def stats():
    """Get visitor statistics"""
    try:
        session = Session()
        total_visits = session.query(VisitorEvent).count()
        recent_visits = session.query(VisitorEvent).order_by(
            VisitorEvent.timestamp.desc()
        ).limit(10).all()

        Session.remove()

        return jsonify({
            "total_visits": total_visits,
            "recent_visits": [
                {
                    "id": v.id,
                    "timestamp": v.timestamp.isoformat(),
                    "endpoint": v.endpoint,
                    "method": v.method,
                    "source_ip": v.source_ip,
                    "response_code": v.response_code
                }
                for v in recent_visits
            ]
        })
    except Exception as e:
        log.error(f"Failed to fetch stats: {e}", exc_info=True)
        return jsonify({"error": "Failed to fetch statistics"}), 500


@app.route("/visits")
def visits():
    """List all visits with pagination"""
    try:
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)

        session = Session()
        query = session.query(VisitorEvent).order_by(VisitorEvent.timestamp.desc())
        total = query.count()
        visits = query.limit(per_page).offset((page - 1) * per_page).all()

        Session.remove()

        return jsonify({
            "page": page,
            "per_page": per_page,
            "total": total,
            "visits": [
                {
                    "id": v.id,
                    "timestamp": v.timestamp.isoformat(),
                    "endpoint": v.endpoint,
                    "method": v.method,
                    "source_ip": v.source_ip,
                    "response_code": v.response_code,
                    "user_agent": v.user_agent
                }
                for v in visits
            ]
        })
    except Exception as e:
        log.error(f"Failed to fetch visits: {e}", exc_info=True)
        return jsonify({"error": "Failed to fetch visits"}), 500


if __name__ == "__main__":
    app.run()
</code></pre>

    <h2>Hypercorn ASGI Configuration</h2>
    <p>Configure Hypercorn for production deployment with multiple workers.</p>
    <p><strong><code>hypercorn.toml</code>:</strong></p>
    <pre><code>
bind = ["0.0.0.0:8000"]
workers = 4
worker_class = "asyncio"
graceful_timeout = 30
keep_alive_timeout = 5
max_requests = 10000
max_requests_jitter = 1000

# Logging
accesslog = "-"
errorlog = "-"
loglevel = "info"

# Performance tuning
backlog = 2048
h11_max_incomplete_event_size = 16384

[ssl]
# Add SSL config if needed
</code></pre>

    <p><strong>Run with Hypercorn:</strong></p>
    <pre><code>
uv run hypercorn app:app --config hypercorn.toml
</code></pre>

    <h2>HAProxy Load Balancer Configuration</h2>
    <p>Configure HAProxy to distribute traffic across multiple application instances.</p>
    <p><strong><code>haproxy.cfg</code>:</strong></p>
    <pre><code>
global
    log stdout format raw local0
    maxconn 4096
    tune.ssl.default-dh-param 2048

defaults
    log     global
    mode    http
    option  httplog
    option  dontlognull
    option  http-server-close
    option  forwardfor except 127.0.0.0/8
    option  redispatch
    retries 3
    timeout connect 5s
    timeout client  50s
    timeout server  50s
    timeout http-request 10s
    timeout http-keep-alive 10s

frontend http_front
    bind *:80
    default_backend http_back

    # Add X-Forwarded-For header
    http-request set-header X-Forwarded-For %[src]

    # Health check endpoint bypass
    acl health_check path /health
    use_backend health_back if health_check

backend http_back
    balance roundrobin
    option httpchk GET /health

    # Add multiple app instances
    server app1 app1:8000 check inter 2s rise 2 fall 3 maxconn 1000
    server app2 app2:8000 check inter 2s rise 2 fall 3 maxconn 1000
    server app3 app3:8000 check inter 2s rise 2 fall 3 maxconn 1000
    server app4 app4:8000 check inter 2s rise 2 fall 3 maxconn 1000

backend health_back
    server app1 app1:8000

# Stats interface
listen stats
    bind *:8404
    stats enable
    stats uri /stats
    stats refresh 30s
    stats show-legends
</code></pre>

    <h2>OpenTelemetry Collector Configuration</h2>
    <p>Configure the OTEL collector to route telemetry to appropriate backends.</p>
    <p><strong><code>otel-collector-config.yaml</code>:</strong></p>
    <pre><code>
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: "0.0.0.0:4317"
      http:
        endpoint: "0.0.0.0:4318"

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024
    send_batch_max_size: 2048

  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  resource:
    attributes:
      - key: deployment.environment
        value: production
        action: upsert

exporters:
  # Jaeger for traces
  otlp/jaeger:
    endpoint: "jaeger:4317"
    tls:
      insecure: true

  # Prometheus for metrics
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: visitor_tracker
    const_labels:
      environment: production

  # Elasticsearch for logs
  elasticsearch:
    endpoints: ["http://elasticsearch:9200"]
    logs_index: "visitor-tracker-logs"
    mapping:
      mode: "none"
    bdi_param_require_data_stream: false
    retry:
      enabled: true
      initial_interval: 1s
      max_interval: 30s
    sending_queue:
      enabled: true
      num_consumers: 20
      queue_size: 50000

  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [otlp/jaeger]

    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [prometheus]

    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [elasticsearch]

  telemetry:
    logs:
      level: info
</code></pre>

    <h2>Backend Configurations</h2>
    <h3>Prometheus Configuration</h3>
    <p><strong><code>prometheus.yml</code>:</strong></p>
    <pre><code>
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'visitor-tracker'
    environment: 'production'

scrape_configs:
  - job_name: 'otel-collector'
    static_configs:
      - targets: ['otel-collector:8889']

  - job_name: 'haproxy'
    static_configs:
      - targets: ['haproxy:8404']

# Alerting rules (optional)
alerting:
  alertmanagers:
    - static_configs:
        - targets: []

# For long-term storage, consider integrating with:
# - Thanos (recommended for multi-cluster setups)
# - Cortex (cloud-native, horizontally scalable)
# - Victoria Metrics (high-performance alternative)
# - Mimir (Grafana's scalable backend)
</code></pre>

    <p><strong>Recommended Backend for Prometheus at Scale:</strong></p>
    <p>For hundreds of nodes, consider using <strong>Thanos</strong> or <strong>Mimir</strong> for long-term storage and
        federation:</p>
    <ul>
        <li><strong>Thanos</strong>: Provides unlimited retention, global query view, and high availability</li>
        <li><strong>Mimir</strong>: Grafana&rsquo;s horizontally scalable Prometheus backend with multi-tenancy</li>
        <li><strong>Victoria Metrics</strong>: High-performance, cost-effective alternative with better compression</li>
    </ul>
    <h3>Jaeger Configuration</h3>
    <p>For production Jaeger deployments, use a proper storage backend instead of in-memory storage.</p>
    <p><strong>Recommended Backends for Jaeger:</strong></p>
    <ul>
        <li><strong>Elasticsearch</strong> (recommended): Best for searching and analyzing traces</li>
        <li><strong>Cassandra</strong>: Better for write-heavy workloads and massive scale</li>
        <li><strong>Kafka</strong>: Use as a buffer between collectors and storage</li>
    </ul>
    <p><strong>Example Jaeger with Elasticsearch backend:</strong></p>
    <pre><code>
docker run -d \
  --name jaeger \
  -e SPAN_STORAGE_TYPE=elasticsearch \
  -e ES_SERVER_URLS=http://elasticsearch:9200 \
  -p 16686:16686 \
  -p 4317:4317 \
  jaegertracing/all-in-one:latest
</code></pre>

    <h3>Elasticsearch Configuration</h3>
    <p><strong><code>elasticsearch.yml</code>:</strong></p>
    <pre><code>
cluster.name: visitor-tracker-logs
node.name: es-node-1
network.host: 0.0.0.0
discovery.type: single-node

# Performance tuning
indices.memory.index_buffer_size: 30%
bootstrap.memory_lock: true

# For production clusters
# discovery.seed_hosts: ["es-node-2", "es-node-3"]
# cluster.initial_master_nodes: ["es-node-1", "es-node-2", "es-node-3"]
</code></pre>

    <p><strong>For scaling Elasticsearch:</strong></p>
    <ul>
        <li>Deploy a 3+ node cluster for high availability</li>
        <li>Use dedicated master nodes for clusters with 10+ data nodes</li>
        <li>Consider using <strong>OpenSearch</strong> as an open-source alternative</li>
        <li>For massive scale, evaluate <strong>Elastic Cloud</strong> or <strong>Amazon OpenSearch Service</strong>
        </li>
    </ul>
    <h2>Async Database Writer Worker</h2>
    <p>To prevent the database from becoming a bottleneck, we decouple request handling from database writes using Redis
        as a queue and a dedicated worker process.</p>
    <p><strong><code>worker.py</code>:</strong></p>
    <pre><code>
import logging
import json
import time
import redis
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from models import VisitorEvent
from datetime import datetime
import os

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://tracker:tracker@postgres:5432/visitor_tracker")
REDIS_URL = os.getenv("REDIS_URL", "redis://redis:6379/0")
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "100"))
BATCH_TIMEOUT = float(os.getenv("BATCH_TIMEOUT", "1.0"))

# Setup database connection
engine = create_engine(
    DATABASE_URL, 
    pool_size=10, 
    max_overflow=20,
    pool_pre_ping=True
)
Session = sessionmaker(bind=engine)

# Setup Redis connection
r = redis.Redis.from_url(REDIS_URL, decode_responses=False)


def process_batch(events):
    """Bulk insert events into database"""
    if not events:
        return

    session = Session()
    try:
        # Bulk insert for better performance
        session.bulk_insert_mappings(VisitorEvent, events)
        session.commit()
        log.info(f"Inserted {len(events)} events into database")
    except Exception as e:
        session.rollback()
        log.error(f"Failed to insert batch: {e}", exc_info=True)
        # Re-queue failed events
        for event in events:
            r.lpush('visitor_events', json.dumps(event, default=str))
    finally:
        session.close()


def worker_loop():
    """Main worker loop - consumes events from Redis and writes to database in batches"""
    log.info("Starting database worker...")
    batch = []
    last_flush = time.time()

    while True:
        try:
            # Non-blocking pop with timeout
            result = r.brpop('visitor_events', timeout=1)

            if result:
                _, event_json = result
                event_data = json.loads(event_json)

                # Convert timestamp string back to datetime if needed
                if isinstance(event_data.get('timestamp'), str):
                    event_data['timestamp'] = datetime.fromisoformat(event_data['timestamp'])

                batch.append(event_data)

            # Flush batch if size or timeout reached
            current_time = time.time()
            should_flush = (
                len(batch) >= BATCH_SIZE or 
                (batch and (current_time - last_flush) >= BATCH_TIMEOUT)
            )

            if should_flush:
                process_batch(batch)
                batch = []
                last_flush = current_time

        except KeyboardInterrupt:
            log.info("Shutting down worker...")
            if batch:
                process_batch(batch)
            break
        except Exception as e:
            log.error(f"Worker error: {e}", exc_info=True)
            time.sleep(1)


if __name__ == "__main__":
    worker_loop()
</code></pre>

    <p><strong>Why this approach eliminates the database bottleneck:</strong></p>
    <ol>
        <li><strong>Asynchronous writes</strong>: Request handling returns immediately after queuing to Redis
            (sub-millisecond operation)</li>
        <li><strong>Batch processing</strong>: Worker writes 100 events at once instead of individual INSERTs</li>
        <li><strong>Decoupled scaling</strong>: Scale workers independently from web servers</li>
        <li><strong>Resilience</strong>: Failed batches are re-queued automatically</li>
        <li><strong>Backpressure handling</strong>: Redis queue acts as a buffer during traffic spikes</li>
    </ol>
    <p><strong>Run multiple workers for high throughput:</strong></p>
    <pre><code>
# Run 4-8 workers depending on database capacity
for i in {1..4}; do
    uv run python worker.py &
done
</code></pre>

    <p><strong><code>Dockerfile</code>:</strong></p>
    <pre><code>
FROM python:3.14-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# Copy uv installer
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Copy project files
COPY pyproject.toml uv.lock ./
COPY . .

# Install dependencies
RUN uv sync --frozen

# Expose port
EXPOSE 8000

# Run with Hypercorn
CMD ["uv", "run", "hypercorn", "app:app", "--config", "hypercorn.toml"]
</code></pre>

    <h2>Load Testing with Locust</h2>
    <p>Create load tests to verify your application scales properly.</p>
    <h2>Load Testing with Multi-Language Tools</h2>
    <h3>The Python Reality Check</h3>
    <p><strong>Hard Truth</strong>: Python-based load testing tools (including Locust with FastHttpUser) hit a ceiling
        around <strong>500-600 QPS per machine</strong> on a 16-core system, even with local connections and async
        calls. This is due to:</p>
    <ol>
        <li><strong>GIL constraints</strong>: Python&rsquo;s Global Interpreter Lock limits true parallelism</li>
        <li><strong>Event loop overhead</strong>: Even gevent/asyncio have significant per-request overhead</li>
        <li><strong>HTTP client limitations</strong>: No production-ready HTTP/2 implementation (httpx is incomplete)
        </li>
        <li><strong>Protocol overhead</strong>: Connection management and parsing dominate at high rates</li>
    </ol>
    <h3>Multi-Language Load Testing Strategy</h3>
    <p>To properly test a high-throughput system, use tools written in compiled languages:</p>
    <p><strong><code>load-testing/locustfile.py</code> (Baseline - Python):</strong></p>
    <pre><code>
from locust import HttpUser, task, between
from locust.contrib.fasthttp import FastHttpUser
import random

class VisitorTrackerUser(FastHttpUser):
    """
    FastHttpUser with gevent - realistic limit: 500-600 QPS per machine
    Use for:
    - Development testing
    - Scenario testing with complex user flows
    - Initial performance baselines
    """
    wait_time = between(0.01, 0.1)

    @task(10)
    def index(self):
        self.client.get("/")

    @task(5)
    def stats(self):
        self.client.get("/stats")

    @task(3)
    def visits_paginated(self):
        page = random.randint(1, 10)
        self.client.get(f"/visits?page={page}&per_page=20")

    @task(1)
    def health_check(self):
        self.client.get("/health")
</code></pre>

    <p><strong>Run Locust (expect 400-600 QPS max per machine):</strong></p>
    <pre><code>
# Single machine baseline
uv run locust -f locustfile.py --host=http://localhost \
    --users 2000 --spawn-rate 100 --run-time 5m --headless

# Distributed across 10 machines = ~5000 QPS total
# Master:
uv run locust -f locustfile.py --master --expect-workers=10

# Workers (on different machines):
uv run locust -f locustfile.py --worker --master-host=<master-ip>
</code></pre>

    <p><strong><code>load-testing/vegeta-targets.txt</code> (Go-based - High throughput):</strong></p>
    <pre><code>
GET http://localhost/
GET http://localhost/stats
GET http://localhost/health
GET http://localhost/visits?page=1&per_page=20
GET http://localhost/visits?page=2&per_page=20
</code></pre>

    <p><strong>Install and run vegeta (10,000+ QPS per machine):</strong></p>
    <pre><code>
# Install vegeta
go install github.com/tsenart/vegeta@latest

# Test at 10,000 QPS sustained for 60 seconds
cat vegeta-targets.txt | vegeta attack -rate=10000 -duration=60s \
    -workers=100 | vegeta report

# Output detailed metrics
cat vegeta-targets.txt | vegeta attack -rate=10000 -duration=60s \
    -workers=100 | vegeta report -type=text

# Generate latency plot
cat vegeta-targets.txt | vegeta attack -rate=10000 -duration=60s \
    -workers=100 | vegeta plot > results.html

# Test maximum throughput (find breaking point)
cat vegeta-targets.txt | vegeta attack -rate=0 -duration=30s \
    -max-workers=200 | vegeta report
</code></pre>

    <p><strong>Expected vegeta performance:</strong>
        - Single machine: 10,000-50,000 QPS depending on target latency
        - Multiple machines: 100,000+ QPS aggregate</p>
    <p><strong><code>load-testing/wrk2-script.lua</code> (C-based - Constant rate):</strong></p>
    <pre><code>
-- wrk2 Lua script for constant-rate testing
wrk.method = "GET"
wrk.headers["Content-Type"] = "application/json"

-- Round-robin across endpoints
endpoints = {"/", "/stats", "/health", "/visits?page=1"}
counter = 0

request = function()
    counter = counter + 1
    local path = endpoints[(counter % #endpoints) + 1]
    return wrk.format(nil, path)
end

-- Track latency distribution
done = function(summary, latency, requests)
    io.write("------------------------------\n")
    io.write(string.format("Requests: %d\n", summary.requests))
    io.write(string.format("Duration: %.2fs\n", summary.duration / 1000000))
    io.write(string.format("Req/sec: %.2f\n", summary.requests / (summary.duration / 1000000)))
    io.write(string.format("Latency p50: %.2fms\n", latency:percentile(50)))
    io.write(string.format("Latency p95: %.2fms\n", latency:percentile(95)))
    io.write(string.format("Latency p99: %.2fms\n", latency:percentile(99)))
end
</code></pre>

    <p><strong>Install and run wrk2 (20,000+ QPS per machine):</strong></p>
    <pre><code>
# Install wrk2
git clone https://github.com/giltene/wrk2.git
cd wrk2 && make

# Constant rate test at 20,000 QPS
./wrk -t 12 -c 400 -d 60s -R 20000 \
    --latency -s wrk2-script.lua http://localhost/

# Find maximum sustainable rate
./wrk -t 16 -c 800 -d 30s -R 50000 \
    --latency -s wrk2-script.lua http://localhost/

# Low latency focus (fewer connections, moderate rate)
./wrk -t 8 -c 200 -d 60s -R 10000 \
    --latency -s wrk2-script.lua http://localhost/
</code></pre>

    <p><strong>Expected wrk2 performance:</strong>
        - Single machine: 20,000-60,000 QPS with accurate latency percentiles
        - Best for validating SLA compliance (p99 &lt; 100ms, etc.)</p>
    <p><strong><code>load-testing/gatling/VisitorTrackerSimulation.scala</code> (JVM-based - Scenarios):</strong></p>
    <pre><code>
import io.gatling.core.Predef._
import io.gatling.http.Predef._
import scala.concurrent.duration._

class VisitorTrackerSimulation extends Simulation {

  val httpProtocol = http
    .baseUrl("http://localhost")
    .acceptHeader("application/json")
    .userAgentHeader("Gatling Load Test")

  // Scenario 1: Normal user behavior
  val normalUser = scenario("Normal User")
    .exec(http("Homepage").get("/"))
    .pause(1.seconds, 3.seconds)
    .exec(http("Stats").get("/stats"))
    .pause(2.seconds, 5.seconds)
    .exec(http("Visits").get("/visits?page=1"))

  // Scenario 2: Heavy user
  val heavyUser = scenario("Heavy User")
    .repeat(10) {
      exec(http("Rapid Homepage").get("/"))
        .pause(100.milliseconds, 500.milliseconds)
    }

  // Scenario 3: Health check monitoring
  val monitoring = scenario("Monitoring")
    .forever {
      exec(http("Health Check").get("/health"))
        .pause(5.seconds)
    }

  setUp(
    normalUser.inject(
      rampUsersPerSec(10) to 1000 during (2.minutes),
      constantUsersPerSec(1000) during (10.minutes)
    ),
    heavyUser.inject(
      rampUsersPerSec(5) to 200 during (2.minutes),
      constantUsersPerSec(200) during (10.minutes)
    ),
    monitoring.inject(
      constantUsersPerSec(10) during (12.minutes)
    )
  ).protocols(httpProtocol)
}
</code></pre>

    <p><strong>Run Gatling (15,000+ QPS per machine, excellent for complex scenarios):</strong></p>
    <pre><code>
# Install Gatling
wget https://repo1.maven.org/maven2/io/gatling/highcharts/gatling-charts-highcharts-bundle/3.10.3/gatling-charts-highcharts-bundle-3.10.3.zip
unzip gatling-charts-highcharts-bundle-3.10.3.zip
cd gatling-charts-highcharts-bundle-3.10.3

# Copy simulation
cp ../VisitorTrackerSimulation.scala user-files/simulations/

# Run test
./bin/gatling.sh -s VisitorTrackerSimulation

# Generate report
# Reports automatically generated in results/ directory
</code></pre>

    <h3>Load Testing Strategy by Scale</h3>
    <table>
        <thead>
            <tr>
                <th>Target QPS</th>
                <th>Tool</th>
                <th>Machines Needed</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>&lt; 500</td>
                <td>Locust</td>
                <td>1</td>
                <td>Development, feature testing</td>
            </tr>
            <tr>
                <td>500-5,000</td>
                <td>Locust distributed</td>
                <td>10</td>
                <td>Baseline load testing</td>
            </tr>
            <tr>
                <td>5,000-50,000</td>
                <td>vegeta</td>
                <td>1-5</td>
                <td>High-throughput validation</td>
            </tr>
            <tr>
                <td>10,000-60,000</td>
                <td>wrk2</td>
                <td>1-3</td>
                <td>Latency SLA validation</td>
            </tr>
            <tr>
                <td>10,000-100,000+</td>
                <td>Gatling</td>
                <td>5-10</td>
                <td>Complex scenarios at scale</td>
            </tr>
            <tr>
                <td>100,000+</td>
                <td>vegeta + wrk2</td>
                <td>10-20</td>
                <td>Stress testing, chaos</td>
            </tr>
        </tbody>
    </table>
    <h3>Distributed Load Testing Architecture</h3>
    <p>For testing beyond 50,000 QPS, distribute load generation:</p>
    <pre><code>
# Coordinator script for distributed vegeta
#!/bin/bash
# run-distributed-load.sh

TARGETS="vegeta-targets.txt"
RATE=50000  # Total rate across all machines
DURATION=300s
WORKERS=(worker1 worker2 worker3 worker4 worker5)

# Calculate rate per worker
RATE_PER_WORKER=$((RATE / ${#WORKERS[@]}))

# Launch vegeta on each worker
for worker in "${WORKERS[@]}"; do
    ssh $worker "cat $TARGETS | vegeta attack -rate=$RATE_PER_WORKER \
        -duration=$DURATION > results-$worker.bin" &
done

wait

# Collect and merge results
for worker in "${WORKERS[@]}"; do
    scp $worker:results-$worker.bin .
done

cat results-*.bin | vegeta report
</code></pre>

    <h3>Reality Check: Expected Performance</h3>
    <p><strong>Python-based tools (Locust):</strong>
        - Single machine: 400-600 QPS (16-core)
        - 10 machines: 4,000-6,000 QPS aggregate
        - <strong>Use for</strong>: Development testing, complex scenarios</p>
    <p><strong>Compiled tools (vegeta, wrk2, Gatling):</strong>
        - Single machine: 10,000-60,000 QPS (16-core)
        - 10 machines: 100,000-600,000 QPS aggregate
        - <strong>Use for</strong>: Production validation, stress testing</p>
    <p><strong>Bottom line</strong>: To test a system designed for 50,000+ QPS, you <strong>must</strong> use compiled
        load testing tools. Python cannot generate sufficient load to properly stress-test your infrastructure.</p>
    <h2>Scaling Considerations</h2>
    <h3>Application Layer</h3>
    <ol>
        <li><strong>Horizontal Scaling</strong>: Add more app instances behind HAProxy</li>
        <li><strong>Connection Pooling</strong>: Configure SQLAlchemy pool size based on worker count</li>
        <li><strong>Async Workers</strong>: Consider using <code>asyncio</code> or <code>trio</code> worker class in
            Hypercorn</li>
        <li><strong>Graceful Shutdowns</strong>: Ensure proper cleanup with <code>graceful_timeout</code></li>
    </ol>
    <h3>Database Layer - Eliminating the Bottleneck</h3>
    <p><strong>Problem</strong>: Synchronous database writes block request handling, creating a severe bottleneck at
        scale.</p>
    <p><strong>Solution</strong>: Three-tier architecture with async writes and read replicas.</p>
    <ol>
        <li><strong>Async write queue</strong>: Redis buffers write operations (covered in worker section)</li>
        <li><strong>Batch processing</strong>: Workers write 100+ events at once to primary database</li>
        <li><strong>Read replicas</strong>: 3+ replicas handle all read queries with round-robin load balancing</li>
        <li><strong>Connection pooling</strong>: Primary has small pool (10), replicas have larger pools (20 each)</li>
        <li><strong>Partitioning</strong>: Partition <code>visitor_events</code> table by timestamp for better query
            performance</li>
        <li><strong>Indexing</strong>: Add indexes on commonly queried fields:</li>
    </ol>
    <pre><code>
CREATE INDEX idx_visitor_events_timestamp ON visitor_events(timestamp DESC);
CREATE INDEX idx_visitor_events_endpoint ON visitor_events(endpoint);
CREATE INDEX idx_visitor_events_source_ip ON visitor_events(source_ip);
</code></pre>

    <p><strong>Performance impact:</strong>
        - Without async queue + ROR: ~500 writes/sec on primary, all queries blocked
        - With async queue + ROR: ~10,000 writes/sec batched to primary, 50,000+ reads/sec distributed across replicas
        - Database is no longer the bottleneck</p>
    <h2>PostgreSQL Read Replica Configuration</h2>
    <p>For read-heavy workloads, configure PostgreSQL streaming replication to offload read queries.</p>
    <p><strong>Primary database configuration (<code>postgresql.conf</code>):</strong></p>
    <pre><code>
# Replication settings
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
synchronous_commit = off  # For better write performance, accept slight risk

# Performance tuning
shared_buffers = 4GB
effective_cache_size = 12GB
maintenance_work_mem = 1GB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
random_page_cost = 1.1
effective_io_concurrency = 200
work_mem = 10MB
min_wal_size = 2GB
max_wal_size = 8GB
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4
</code></pre>

    <p><strong>Create replication user:</strong></p>
    <pre><code>
CREATE ROLE replicator WITH REPLICATION PASSWORD 'secure_password' LOGIN;
</code></pre>

    <p><strong><code>pg_hba.conf</code> (allow replication connections):</strong></p>
    <pre><code>
host replication replicator replica_ip/32 md5
</code></pre>

    <p><strong>On replica server, create recovery configuration:</strong></p>
    <pre><code>
# Stop replica PostgreSQL
systemctl stop postgresql

# Remove data directory and clone from primary
rm -rf /var/lib/postgresql/18/main/*
pg_basebackup -h primary_ip -D /var/lib/postgresql/18/main -U replicator -P -v -R -X stream -C -S replica_slot

# Start replica
systemctl start postgresql
</code></pre>

    <p><strong>Update application to use read replicas:</strong></p>
    <p><strong><code>database.py</code>:</strong></p>
    <pre><code>
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, scoped_session
from contextlib import contextmanager

# Primary database for writes
PRIMARY_URL = "postgresql://tracker:tracker@postgres-primary:5432/visitor_tracker"
# Read replicas for queries
REPLICA_URLS = [
    "postgresql://tracker:tracker@postgres-replica-1:5432/visitor_tracker",
    "postgresql://tracker:tracker@postgres-replica-2:5432/visitor_tracker",
    "postgresql://tracker:tracker@postgres-replica-3:5432/visitor_tracker",
]

# Write engine
write_engine = create_engine(
    PRIMARY_URL,
    pool_size=10,
    max_overflow=20,
    pool_pre_ping=True
)

# Read engines (round-robin)
read_engines = [
    create_engine(
        url,
        pool_size=20,
        max_overflow=40,
        pool_pre_ping=True
    )
    for url in REPLICA_URLS
]

WriteSession = scoped_session(sessionmaker(bind=write_engine))

import itertools
read_engine_cycle = itertools.cycle(read_engines)

def get_read_session():
    """Get session from next read replica in round-robin"""
    engine = next(read_engine_cycle)
    return scoped_session(sessionmaker(bind=engine))


@contextmanager
def write_session():
    """Context manager for write operations"""
    session = WriteSession()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        WriteSession.remove()


@contextmanager
def read_session():
    """Context manager for read operations"""
    Session = get_read_session()
    session = Session()
    try:
        yield session
    finally:
        Session.remove()
</code></pre>

    <p><strong>Update Flask routes to use read replicas:</strong></p>
    <pre><code>
@app.route("/stats")
def stats():
    """Get visitor statistics - uses read replica"""
    try:
        with read_session() as session:
            total_visits = session.query(VisitorEvent).count()
            recent_visits = session.query(VisitorEvent).order_by(
                VisitorEvent.timestamp.desc()
            ).limit(10).all()

        return jsonify({
            "total_visits": total_visits,
            "recent_visits": [
                {
                    "id": v.id,
                    "timestamp": v.timestamp.isoformat(),
                    "endpoint": v.endpoint,
                    "method": v.method,
                    "source_ip": v.source_ip,
                    "response_code": v.response_code
                }
                for v in recent_visits
            ]
        })
    except Exception as e:
        log.error(f"Failed to fetch stats: {e}", exc_info=True)
        return jsonify({"error": "Failed to fetch statistics"}), 500


@app.route("/visits")
def visits():
    """List all visits with pagination - uses read replica"""
    try:
        page = request.args.get('page', 1, type=int)
        per_page = request.args.get('per_page', 50, type=int)

        with read_session() as session:
            query = session.query(VisitorEvent).order_by(VisitorEvent.timestamp.desc())
            total = query.count()
            visits = query.limit(per_page).offset((page - 1) * per_page).all()

        return jsonify({
            "page": page,
            "per_page": per_page,
            "total": total,
            "visits": [
                {
                    "id": v.id,
                    "timestamp": v.timestamp.isoformat(),
                    "endpoint": v.endpoint,
                    "method": v.method,
                    "source_ip": v.source_ip,
                    "response_code": v.response_code,
                    "user_agent": v.user_agent
                }
                for v in visits
            ]
        })
    except Exception as e:
        log.error(f"Failed to fetch visits: {e}", exc_info=True)
        return jsonify({"error": "Failed to fetch visits"}), 500
</code></pre>

    <p><strong>Why Read Replicas Eliminate Database Bottleneck:</strong></p>
    <ol>
        <li><strong>Write isolation</strong>: Primary handles writes only, no read contention</li>
        <li><strong>Horizontal read scaling</strong>: Add more replicas as read load increases</li>
        <li><strong>Geographic distribution</strong>: Place replicas closer to users</li>
        <li><strong>Load distribution</strong>: Round-robin across replicas spreads load evenly</li>
        <li><strong>Fault tolerance</strong>: Reads continue even if one replica fails</li>
    </ol>
    <p><strong>With this architecture:</strong>
        - Primary: Handles async batch writes from workers (low load)
        - Replicas: Handle all read queries from web servers (distributed load)
        - Result: Database is no longer a bottleneck, can scale to millions of requests</p>
    <h3>OTEL Collector Scaling</h3>
    <ol>
        <li><strong>Deploy multiple collectors</strong>: Use HAProxy or Kubernetes services to load balance</li>
        <li><strong>Resource limits</strong>: Adjust <code>memory_limiter</code> based on available resources</li>
        <li><strong>Batch sizes</strong>: Tune <code>batch</code> processor for optimal throughput vs latency</li>
        <li><strong>Queue sizes</strong>: Increase <code>sending_queue.queue_size</code> for high-volume scenarios</li>
    </ol>
    <h3>Monitoring Your Stack</h3>
    <p><strong>Key Metrics to Watch:</strong></p>
    <ul>
        <li><strong>Application</strong>: Request rate, latency (p50, p95, p99), error rate</li>
        <li><strong>HAProxy</strong>: Backend health, connection rate, queue depth</li>
        <li><strong>Database</strong>: Connection pool usage, query duration, deadlocks</li>
        <li><strong>OTEL Collector</strong>: Queue depth, dropped spans/metrics/logs, export errors</li>
        <li><strong>Prometheus</strong>: Ingestion rate, query duration, storage size</li>
        <li><strong>Jaeger</strong>: Trace ingestion rate, storage backend health</li>
        <li><strong>Elasticsearch</strong>: Indexing rate, search latency, cluster health</li>
    </ul>
    <p><strong>Grafana Dashboards:</strong></p>
    <p>Import these community dashboards for instant visibility:</p>
    <ul>
        <li>HAProxy: Dashboard ID <code>367</code></li>
        <li>PostgreSQL: Dashboard ID <code>9628</code></li>
        <li>OTEL Collector: Dashboard ID <code>15983</code></li>
        <li>Flask Application: Create custom dashboard with your metrics</li>
    </ul>
    <h2>Performance Tuning Tips</h2>
    <ol>
        <li><strong>Database Architecture</strong>: </li>
    </ol>
    <ul>
        <li>Use async writes with Redis queue to decouple request handling</li>
        <li>Deploy 3+ read replicas with round-robin load balancing</li>
        <li>Primary handles only batched writes, replicas handle all reads</li>
        <li>Total capacity: <code>(replicas * replica_qps) + (primary_batch_write_throughput)</code></li>
    </ul>
    <ol start="2">
        <li><strong>HTTP Protocol &amp; Load Testing Reality</strong>: </li>
    </ol>
    <ul>
        <li>Python tools (Locust, even with FastHttpUser): <strong>500-600 QPS max per 16-core machine</strong></li>
        <li>This is due to GIL, event loop overhead, and lack of production HTTP/2</li>
        <li>For high-throughput testing, use compiled tools:<ul>
                <li><strong>vegeta (Go)</strong>: 10,000-50,000 QPS per machine</li>
                <li><strong>wrk2 (C)</strong>: 20,000-60,000 QPS per machine </li>
                <li><strong>Gatling (JVM)</strong>: 15,000-40,000 QPS per machine</li>
            </ul>
        </li>
        <li>Distributed Locust (10 machines): ~5,000 QPS total</li>
        <li>Distributed vegeta (10 machines): ~300,000 QPS total</li>
    </ul>
    <ol start="3">
        <li><strong>Connection Management</strong>: </li>
    </ol>
    <ul>
        <li>Primary DB: small pool (10) since only workers connect</li>
        <li>Replica DB: larger pools (20) since web servers connect</li>
        <li>Redis: connection pooling enabled by default</li>
        <li>Total connections: <code>(app_instances * workers * 2) + (worker_processes * 10)</code></li>
    </ul>
    <ol start="4">
        <li><strong>Batch Processing</strong>: </li>
    </ol>
    <ul>
        <li>Increase worker batch sizes (100-1000) for higher write throughput</li>
        <li>Tune batch timeout (0.5-2s) based on latency requirements</li>
        <li>Run multiple workers (4-8) to parallelize database writes</li>
    </ul>
    <ol start="5">
        <li><strong>Caching Layer</strong>: </li>
    </ol>
    <ul>
        <li>Add Redis caching for <code>/stats</code> endpoint (refresh every 30s)</li>
        <li>Cache read replica query results for frequently accessed data</li>
        <li>Implement stale-while-revalidate pattern for better perceived performance</li>
    </ul>
    <ol start="6">
        <li><strong>Load Testing Strategy</strong>:</li>
    </ol>
    <ul>
        <li><strong>Development/baseline</strong>: Locust (Python) - 400-600 QPS per machine</li>
        <li><strong>Production validation</strong>: vegeta (Go) - 10,000-50,000 QPS per machine</li>
        <li><strong>SLA validation</strong>: wrk2 (C) - 20,000-60,000 QPS with accurate percentiles</li>
        <li><strong>Complex scenarios</strong>: Gatling (JVM) - 15,000-40,000 QPS with stateful flows</li>
        <li><strong>Stress testing</strong>: Distributed compiled tools across 10-20 machines for 100,000+ QPS</li>
    </ul>
    <h2>Observability Best Practices</h2>
    <ol>
        <li><strong>Correlation IDs</strong>: Ensure trace IDs propagate through all services</li>
        <li><strong>Structured Logging</strong>: Use JSON logging for better parsing in Elasticsearch</li>
        <li><strong>Context Propagation</strong>: OpenTelemetry automatically handles this with instrumentation</li>
        <li><strong>Error Tracking</strong>: Integrate with Sentry for application error monitoring</li>
        <li><strong>Alerting</strong>: Set up alerts in Prometheus for critical metrics (error rates, latency)</li>
    </ol>
    <h2>Conclusion</h2>
    <p>This architecture provides a production-ready, horizontally scalable observability stack that can handle hundreds
        of application nodes and sustain 50,000+ QPS. The key architectural decisions eliminate common bottlenecks:</p>
    <p><strong>Database Bottleneck Eliminated:</strong></p>
    <ul>
        <li><strong>Async writes via Redis</strong>: Request handling never blocks on database writes (&lt; 1ms queue
            operation)</li>
        <li><strong>Batch processing</strong>: Workers achieve 10,000+ writes/sec with bulk inserts (100 events/batch)
        </li>
        <li><strong>Read replicas (ROR)</strong>: Distributed read load across 3+ replicas for 50,000+ reads/sec</li>
        <li><strong>Result</strong>: Database scales linearly with replica count, primary handles only batched writes
        </li>
    </ul>
    <p><strong>Load Testing Reality:</strong></p>
    <ul>
        <li><strong>Python tools (Locust)</strong>: Hard limit of 500-600 QPS per 16-core machine due to GIL and
            protocol overhead</li>
        <li><strong>Compiled tools required</strong>: vegeta (Go), wrk2 (C), Gatling (JVM) achieve 10,000-60,000 QPS per
            machine</li>
        <li><strong>Distributed testing</strong>: 10 machines with vegeta = 300,000+ QPS aggregate vs 5,000 QPS with
            Locust</li>
        <li><strong>Critical insight</strong>: You cannot properly test a 50k+ QPS system with Python-based tools</li>
    </ul>
    <p><strong>Comprehensive Observability:</strong></p>
    <ul>
        <li><strong>OpenTelemetry</strong> standardizes telemetry collection across all services</li>
        <li><strong>HAProxy</strong> distributes load efficiently with health checking</li>
        <li><strong>Hypercorn</strong> provides high-performance ASGI serving with multiple workers</li>
        <li><strong>PostgreSQL with ULID</strong> provides distributed-friendly data storage</li>
        <li><strong>Prometheus/Jaeger/Elasticsearch</strong> offer specialized backends for each telemetry type</li>
    </ul>
    <p><strong>Architecture Diagram Summary:</strong></p>
    <pre>
Load Gen (Go/C/JVM) â†’ HAProxy â†’ Flask Apps â†’ Redis Queue â†’ Workers â†’ PostgreSQL Primary
                                      â†“                                         â†“
                                 OTEL Collector                        Read Replicas
                                      â†“                                         â†‘
                          Prometheus/Jaeger/ES                        Flask Apps (reads)
</pre>

    <p>For scaling beyond hundreds of nodes and 100k+ QPS, consider:</p>
    <ul>
        <li><strong>Kubernetes</strong> for orchestration with HPA (Horizontal Pod Autoscaling)</li>
        <li><strong>Service mesh</strong> (Istio/Linkerd) for advanced traffic management and automatic observability
        </li>
        <li><strong>Distributed tracing sampling</strong> (1-10%) to reduce overhead at extreme scale</li>
        <li><strong>Managed services</strong>: Grafana Cloud (Prometheus), Elastic Cloud (Elasticsearch), AWS RDS
            (PostgreSQL)</li>
        <li><strong>Kafka</strong> as a buffer layer between OTEL collectors and backends for massive scale</li>
        <li><strong>TimescaleDB</strong> for visitor_events table (better compression and time-series query performance)
        </li>
        <li><strong>Content delivery</strong>: CloudFlare/Fastly for edge caching and DDoS protection</li>
    </ul>
    <p>The combination of async writes, read replicas, and realistic load testing with compiled tools ensures your
        observability stack can handle real-world production loads. The architecture is proven to scale, but remember:
        <strong>always load test with tools that can actually generate the required throughput</strong>. Python tools
        are excellent for development and complex scenarios, but compiled tools are mandatory for validating
        high-throughput production systems.</p>
</body>

</html>