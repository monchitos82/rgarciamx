### Understanding quantum computers (04/25)

I have to start by saying that I know nothing about quantum computers or even understand Grover's algorithm, or for that matter any other; I am super dumb at math, simple as that. I know shit about _qubits_ and the medium that is actually useful for packing them. So yeah, I know nothing. I often watch people talking about quantum computers and how these machines are powerful because they can compute larger numbers faster and that _qubits_ are both _digital_ states at the same time. Yet, I don't think people creating content talking about quantum computers actually understand how they work, so in a nutshell, just like me, they know shit. They just pretend they do and that sucks for the rest of us, because we don't get to _see_ the beauty of how these machines really work.

So trying to shed some light into my own ignorance I was curious if I could actually figure out how quantum computers work and while I got a grasp on it, my explanation still has gaps and it is imperfect, so if you're reading this, you know by now I am no expert nor pretend to be one and this explanation I share just to keep this _imperfect_ understanding alive.

So trying to be the simplest, these computers use _qubits_ to represent data. Their _qubits_ (complex circuits containing particles that can represent quantum states _wuuuut?_) are able to represent all possible answers (all possible combinations in a word) at the same time. _Qubits_ may contain both states of information (_superposition_) so in 2 _qubits_ we can have a word that is both 00, 01, 10 and 11. To make it even more intriguing, _qubits_ often are entangled, thus, these 2 "bits" will contain all combinations when you only need to measure one to infer the values on the other. To make this even more interesting, you can have way larger and more complex words with less _qubits_ compared to digital representation of bits, since the words are 2‚Åø (where _n_ is the number of _qubits_) unlike with conventional binary words where you "only" have _n_ bits word size... it sounds the same, but it isn't, let's say for 4 bits, traditionally you'd have a word size of 4 (yes you can combine and reach 16 possible values, but not at the same time) while in a quantum approach you'd have all combos of the states in that number of bits, thus you end up with 16 representations at once.

This is not the only advantage, and it is not advantage at all, having all possible values means nothing if you don't know what value is the one you're trying to represent, or in the practice to calculate. This is the part that often people omit to add into their explanation and in my opinion, the one that puts all the sense into the _how this shit works_. If you measure a computation (operation), in a digital (silicon based) computer you run some gateways and measure the result looking at what is on and off (what bits are 1's and 0's). Quantum machines are less simplistic, they have all results at the same time, so in order to figure out what result is the one that is true, the results are measured more than once. This is where this gets fascinating, a _qubit_ is anything until it is measured, then it collapses (enters one state and only one) to what is *probably* truth. Upon a constant number of measurements, _qubits_ will collapse into what is most likely the right answer, so the word with the highest count of occurrences is the one with the highest probability of being true... Jeez, so this is probabilistic? As I get it: Yes.

But who measures, where is this all being computed? How are all the pieces connected?

That is the part that I had the hardest time understanding, because quantum computers need digital computers to measure results, they are not replacements of each other. One holds the information, the other measures and computes the results to determine what is more likely to be truth. So _qubits_ get signals? My understanding is that they get influenced by gateways just like a transistor would get a signal to produce a result, but their state is affected by external factors (particles, temperature, affecting their entanglement, magnetic changes) and those produce changes in the values they represent once measured. Is this accurate? I don't know, as I said, I am no expert, my understanding is that it kind of is correct, yet the measurement is the other part of the equation/chain, where other external factors like lasers doing the observation and feeding it into digital computers close the loop on a measurement.

Fascinating? Maybe, I wish my understanding was deeper so I could put this yet in a simpler explanation, but if I could summarize, it would go this way:

You have entangled particles in a medium (often supercooled) that is fed signals. These particles are part of a logical gateway that will produce a result once measured. This operation is going to be repeated several times and the measurement will be aggregated into a probabilistic array where the highest probability fed into an algorithm will confirm a response. This is not super fast, it is as fast as the measuring computers get, however it is extremely parallel and that is where the advantage in front of a traditional computer lies, you can measure all paths taking you to a result, and those results in a few iterations will get a trustworthy value that otherwise would require a constant iteration, aggregation and incremental evaluation in a sequential fashion that would take a longer time to produce a similar answer.