### Containers (06/25)

I have to start here by saying I am no expert in containers. I have been around to watch and participate in the transition from _VMs_ to containers. I have used many and yet not enough of them. This entry is another one of my tiny ass cheat sheet entries by the way.

Anyway, my interest in virtualization and hypervisors dates a couple decades back. I started when vmware was the shit and back then I had no clue what to use it for. I recall having a client running in my budget machine and never delving into what I could do with it because my understanding of _Linux_ was more than limited. Then I graduated college, started delving into _Solaris_ and started hating to do all my work via _putty_ so I ended up finding a way to use virtualbox and a USB to get a terminal and an _IDE_ that actually felt comfortable rather than using _notepad++_ and _putty_ to get things done. So back then all I understood about isolation were permissions and root privileges, never thought (or learned) about chroot and what a good tool it was to break apart an application path, so for sure _cgroups_ were out of the map.

Then _docker_ came along and it was pure magic, the idea of having an image where you could install, test and ship your code was very exciting, specially when most of my work was done by a playbook, so having a way to automate the build and a way to ship in a tested manner where no installation was required felt like a big time saver. But then one wonders, what is this container actually doing and why do I still use virtualbox anyway? That was a question it took me some time to answer. Articles and demos all mentioned something about borrowing what is part of the kernel, but never mentioned which parts. Well, that's where that explanation sucks because to be fair nothing is borrowed, in the simplest term, a container is just creating a partition within the _OS_ resources to create isolation. Had this been told just like that the concept would be super simple to digest, but you know, people like to sound smart.

I think I never pulled apart from the abstraction _docker_ created and what actually was going on underneath until I stumbled upon a place where containers were self managed! What? You don't use _docker_ for that? It took no time to see _docker_ falling out of fashion and kubernetes ramping up, still, the notion of what is being **used** from the _OS_ itself was intriguing without understanding what is `libvirt`, what's _LXC_, what are _namespaces_ and _cgroups_. So let's try to dig into those things, shall we?

I start with `libvirt` because it is a quite useful _API_ to communicate the _OS_ with different hypervisors, not to mention giving us tools for management of resources. This comes handy when we try to understand the concept of an orchestrator, although this might feel low level, the tooling and the API we have to work with hypervisors, including _LXC_ is key for managing what the container platform creates. I don't think the mentioned library has much to do (if anything) with containers, still it is worth mentioning, because part of the need for containers comes from the need to communicate to parts of the kernel through an API like this one.

Moving on to _LXC_ and why using it paired with `libvirt` can be useful to get a similar functionality to _Docker_, we need to understand what the _Linux_ containers project actually is. _LXC_ is not really what introduced the isolation it is going to work with, but it has a nice way to abstract it so we can focus on defining resources and types without delving into what components of the kernel itself is leveraging to provide isolation for an application. No wonder _docker_ started building around it when it started. But trying to dig deeper, can we do our own container by hand?

Well, circling back to the "self managed" mention, yes: Isolation rather than virtualization (we should always make that distinction when we talk about containers) means that we provide the mechanisms from the kernel itself for an application to run without any awareness of what runs outside its restricted space. How is that achieved? through a set of features from the kernel, of course: capabilities, _cgroups_, _namespaces_, policies and _filesystem_ isolation (chroot comes into play again).

But why or how does this work?

Let's think about an application that requires a certain set of libraries, directories, processes and calls to run. If we already have a kernel that provides its scheduler and sys-calls, we don't really need to worry about that layer. If we however try to use _PID_ 1 for this application and limit what runs on the _OS_ to a given process, we need a way to let that application "_believe_" it is the only one running. Using concepts like _namespaces_ and _cgroups_ that can be achieved, _namespaces_ will _tag_ a set of resources to be used for a specific process and limit its view, what the process can see, to what _namespaces_ has _tagged_ (in a _OS_ structure point of view, this includes the _PID_ tree, networking, ipc, hostname, user mapping and mounting points just to name some, depending on what you define using `unshare`) while _cgroups_ will reserve and limit the resources that are provisioned to the process (you need `cgroup-tools` to get this working since you need a cgroup created using `cgcreate` and add resources using `cgset`). Is this enough to contain a process? To some extent it is, however the restriction is not limiting access to sys-calls and other critical parts of the _OS_, thus, the use of capabilities and _apparmor_ policies are needed not only to isolate a process, but to restrict it from operating beyond its own scope (we can see how this one works using `showpcaps` on a _PID_ and set it using `capsh`).

So is _docker_ needed at all? What about _kubernetes_?

Well, _docker_, just like _LXC_ abstracts and simplifies the operation with containers, just like libvirt and the _docker_ daemon simplify the orchestration at host level. Why is this the first time I mention _kubernetes_? Because _kubernetes_ is not really part of this orchestration, it lives and operates on a different layer, where groups of containers (or machines) are the abstracted resources so basically executing this same concept at a different scale of resources.

In my experience, all that is needed, all the "magic" is provided by the _Linux_ kernel. We are sold this as a breakthrough but this magic has been around since 2002 and widely adopted in 2008. I have used "in-house crafted" versions of these orchestrators and containerization platforms, they worked and worked quite well. Am I suggesting not using _docker_ or _kubernetes_? Not at all, these platforms are convenient, especially paired with cloud providers, however, they mean complexity just as using AWS means complexity managing resources through a non-standard _API_, yet they are great enablers for scalability when done right, and a strong implementation of what these concepts propose on resource isolation.

Other parts I should mention?

Sure: _UFS_! Although not necessary, the _union-filesystem_ is a layered approach where space is managed with the intent to prevent copying and duplicating files across containers. This provides not only access to different _filesystems_ through a unified view but it allows access to parts of the container like the image where files (read only) can be accessed by different containers, that's why it is a key part of _docker_, however, it is not an exclusive approach of a layered access _Fylesystem_ for user spaces, _LXC_ has its own implementation, however the key purpose of each is different.

And that's it, containers are all about local isolation. Hypervisors are a different animal, they implement their own microcode, drivers and access to resources so while they can run on top of another _OS_, their purpose is to provide an isolated platform for an _OS_ itself. And this just took me years to understand and explain.

What do I use? Well I use what suits my needs, _podman_ for the time being.