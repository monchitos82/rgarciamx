### Statistics can be a dangerous trap (02/26)

As someone who cares about metrics, it is obvious statistics are important to me. Until I started living in a world of metrics, alerts and performance, statistics were an occasional thought, regardless, they are everywhere.

Politicians love playing with them just like any sales person and what seems to be a very interesting world of numbers and their meaning turns into a dangerous trap. Thus being smart about them is our best defense.

This post is no lecture on statistics, simply it is a list of some examples for us to think things thoroughly for the sake of nurturing our critical thinking.

1. The beef vs AI trap. It is a no-brainer that beef as an industry is resource hungry. It has been demonstrated that the consumption of land, water, energy to produce a single patty of beef is massive. The byproduct and waste generated by this food production cycle is not better. Still, food has to make it to our table.
AI by itself is not very demanding of resources according to what universities and AI companies have published. The trap here, is comparing a production process to a service of inference. Normally, numbers will favor the service since they only cover the operational costs, but for the service to exist, there is an entire production process being dismissed, this eliminates the _apples to apples_ approach. This example is not meant to defend the beef industry (the industry cherry picks their metrics too), simply stresses the need for extending the context rather than using numbers selectively.

1 lb of beef needs 1,850 gallons of water, 1 query in a top-tier LLM model needs 0.01 gallons... except it dismisses the production of a cluster of 10,000 GPUs with a similar lifespan of that from a cow, the building of a datacenter, extraction of metals for the rack, etc. It just focuses on power and cooling... well, that's not right, is it?

2. The "security improvements" trap. Politicians love to present numbers to demonstrate their progress, yet they selectively use means rather than medians and compare samples vs. aggregates to affect the signal they present. If you take the sum of violent crime events and you compare them to the moving average or just a different time window within your administration (let's say a quarter), then the sum will massively outnumber what you present, now throw a bar chart and boom! you suddenly outshine the competitors.

In Mexico, politicians love to show their progress by selectively showing data, e.g. daily violent death statistics or nowadays in a "more sophisticated" way: just tagging ~30-60% as missing people. This indeed produces lower death rates in comparison, but it also demonstrates their lack of brains, if you compare your accumulation of 100,000 deaths vs the previous administration total of 120,000, definitely the current number is lower, except when you look that the aggregation is only spanning over 3 years, vs. a 6 year span from the other. Even in a _linear_ growth, current's accumulation would add up to 200,000 by the 6th year.

3. The latency trap. Latency metrics are very, very misleading when not properly understand. Let's start by something simple: metrics collection rarely will use a 100% sampling. That means, whatever you see, will already be manipulated by the time you run your analysis. Even if not, your options of analysis could be: using averages (very wrong), using percentiles (still misleading), using histograms (hard to break down right), or using individual datapoints (very hard to analyze and attribute). Latency normally tends to present itself within a normal distribution curve, but outliers are trickier than you think. Long tails can create the pareto of a pareto principle, you end up looking for multiple needles in multiple haystacks.

If you wrongly look at the average latency of a service being 500ms, then your latency might look ok, but what if you unwrap the data? If sampled, that means the average of 20, maybe 30% of all requests. It also skews the values where 8 requests respond in 500ms while 1 could take 100ms and another close to 2 seconds. If you look at percentiles is not much better if you look at the wrong one, by this same example the P90 is 500ms, except in the long tail, you have a response that is close to 4x the rest. In real life examples, that number can shoot up to minutes and you wouldn't even notice... let alone what might happen with the 23-40 (_depending on the sample_) requests you didn't measure.

So, please, if you read this, next time you are presented with statistic numbers, be ready to shoot some questions, be ready to dig for details, keep your calculator close and your criticism closer.